{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ7i1HkmYY68"
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rYVufslYY7D"
   },
   "source": [
    "В этом ноутбуке мы будем тренировать автоэнкодеры кодировать лица людей. Для этого возьмем следующий датасет: \"Labeled Faces in the Wild\" (LFW) (http://vis-www.cs.umass.edu/lfw/). Код для скачивания и загрузки датасета написан за вас в файле get_dataset.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wru2LNFuL2Iq"
   },
   "source": [
    "# Vanilla Autoencoder (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr3STtdpYY7G"
   },
   "source": [
    "## Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTNi9JLRYY7I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W3KhlblLYY7P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images not found, donwloading...\n",
      "extracting...\n",
      "done\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e80deba01342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mget_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/Deep_Learning_school/project/get_dataset.py\u001b[0m in \u001b[0;36mfetch_dataset\u001b[0;34m(attrs_name, images_name, dx, dy, dimx, dimy)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tar xvzf tmp.tgz && rm tmp.tgz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
    "# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
    "from get_dataset import fetch_dataset\n",
    "data, attrs = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MSzXXGoYY7X"
   },
   "source": [
    "\n",
    "Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFc8lTm_YY7Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9CC-DUhYY7i"
   },
   "source": [
    "## Autoencoder\n",
    "В этом разделе мы напишем и обучем обычный автоэнкодер.\n",
    "\n",
    "Надеюсь, что к этому моменту вы уже почитали про автоэнкодеры и знаете, зачем они нужны и какова их архитектура. Если нет, то начните с этих ссылок: \n",
    "\n",
    "https://habr.com/ru/post/331382/ \\\\\n",
    "https://towardsdatascience.com\\intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/nVJAtMT.png\" alt=\"Autoencoder\">\n",
    "\n",
    "^ вот так выглядит автоэнкодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csrNCYh-YY7j"
   },
   "outputs": [],
   "source": [
    "dim_code = <your code here> # выберите размер латентного вектора, т.е. code, самой \"узкой\" части автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjr-N8AWee-k"
   },
   "source": [
    "Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SjHNX-rYY7k"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        <определите архитектуры encoder и decoder>\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        <реализуйте forward проход автоэнкодера\n",
    "        в качестве ваозвращаемых переменных -- латентное представление картинки (latent_code) \n",
    "        и полученная реконструкция изображения (reconstruction)>\n",
    "        \n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73lg3bI2YY7m"
   },
   "outputs": [],
   "source": [
    "criterion = <loss>\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "optimizer = <Ваш любимый оптимизатор>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdxg_3WJYY7o"
   },
   "source": [
    "Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n",
    "\n",
    "А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H3DOojrYY7o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код тренировки автоэнкодера>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAztAMA4YY7q"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1J__yvxYY7r"
   },
   "outputs": [],
   "source": [
    "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OPh9O6UYY7s"
   },
   "source": [
    "Not bad, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFi96giuYY7t"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOtUaPNYYY7t"
   },
   "source": [
    "Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n",
    "\n",
    "Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n",
    "\n",
    "#### If that doesn't work\n",
    "Если вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как np.random.randn(25, <latent_space_dim>). А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как лаьентные векторы реальных фоток. Так что ридется рандом подогнать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IZykARRYY7u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сгенерируем 25 рандомных векторов размера latent_space\n",
    "z = np.random.randn(25, <latent_space_dim>)\n",
    "output = <скормите z декодеру>\n",
    "<выведите тут полученные картинки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ey8dD9s0YY7w"
   },
   "source": [
    "## Congrats!\n",
    "\n",
    "Time to make fun!\n",
    "\n",
    "Давайте научимся пририсовывать людям улыбки =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1v-8WwuYY7w"
   },
   "source": [
    "<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGE0M2GDYY7x"
   },
   "source": [
    "План такой:\n",
    "\n",
    "1) Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15 сойдет) людей с улыбками и столько же без.\n",
    "\n",
    "Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n",
    "\n",
    "2) Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n",
    "\n",
    "3) Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n",
    "\n",
    "3) А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного чувака и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1oBX9EeYY7x"
   },
   "outputs": [],
   "source": [
    "<а вот тут все это надо запрогать, да>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXI6jprOYY7z"
   },
   "source": [
    "Вуаля! Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UAf0bpYY70"
   },
   "source": [
    "Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в lwf_deepfinetuned.txt =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQnEGmknYY71"
   },
   "source": [
    "# Variational Autoencoder. (3 балла) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWQNRjJq2uTz"
   },
   "source": [
    "Представляю вам проапгрейдженную версию автоэнкодеров -- вариационные автоэнкодеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb5AHMcxYY73"
   },
   "source": [
    "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoNVT5tYYY74"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        <определите архитектуры encoder и decoder\n",
    "        помните, у encoder должны быть два \"хвоста\", \n",
    "        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n",
    "\n",
    "    def encode(self, x):\n",
    "        <реализуйте forward проход энкодера\n",
    "        в качестве ваозвращаемых переменных -- mu и logsigma>\n",
    "        \n",
    "        return mu, logsigma\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        <реализуйте forward проход декодера\n",
    "        в качестве ваозвращаемой переменной -- reconstruction>\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        <используя encode и decode, реализуйте forward проход автоэнкодера\n",
    "        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n",
    "        return mu, logsigma, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAB77d-PYY76"
   },
   "source": [
    "Определим лосс и его компоненты для VAE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxJrkXGQo5bp"
   },
   "source": [
    "Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n",
    "\n",
    "Общий лосс будет выглядеть так:\n",
    "\n",
    "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
    "\n",
    "Формула для KL-дивергенции:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
    "\n",
    "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac5ey7uIYY77"
   },
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
    "    \"\"\"\n",
    "    loss = <напишите код для KL-дивергенции, пользуясь формулой выше>\n",
    "    return \n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n",
    "    \"\"\"\n",
    "    loss = <binary cross-entropy>\n",
    "    return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, mu, logsigma, reconstruction):\n",
    "    return <соедините тут две компоненты лосса. Mind the sign!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPJQu70eYY79"
   },
   "source": [
    "И обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtCjfqXdYY79"
   },
   "outputs": [],
   "source": [
    "criterion = loss_vae\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "optimizer = <Ваш любимый оптимизатор>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY1khca6YY7_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<обучите модель, как и autoencoder, но на датасете MNIST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkxW_8fkYY8B",
    "scrolled": true
   },
   "source": [
    "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jd3BWM_YY8C"
   },
   "outputs": [],
   "source": [
    "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0jI-FKEYY8E"
   },
   "source": [
    "And finally sample from VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po-KV8E8YY8F"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQXYIXjoYY8F"
   },
   "source": [
    "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhhH-osYY8G"
   },
   "outputs": [],
   "source": [
    "# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n",
    "z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\n",
    "output = <скормите z декодеру>\n",
    "<выведите тут полученные картинки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzt-ENxCr6ul"
   },
   "source": [
    "## Latent Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIWy670xr-Uv"
   },
   "source": [
    "Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\n",
    "Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n",
    "\n",
    "Это позволит оценить, насколько плотно распределены латентные векторы лиц в пространстве. \n",
    "\n",
    "Плюс давайте сделаем такую вещь: у вас есть файл с атрибутами lwf_deepfinetuned.txt, который скачался вместе с базой картинок. Там для каждой картинки описаны атрибуты картинки (имя человека, его пол, цвет кожи и т.п.). Когда будете визуализировать точки латентного пространства на картинке, возьмите какой-нибудь атрибут и покрасьте точки в соответствии со значем атрибута, соответствующего этой точке. \n",
    "\n",
    "Например, возьмем атрибут \"пол\". Давайте покрасим точки, которые соответствуют картинкам женщин, в один цвет, а точки, которые соответствуют картинкам мужчин -- в другой.\n",
    "\n",
    "Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n",
    "\n",
    "\n",
    "Итак, план:\n",
    "1. Получить латентные представления картинок тестового датасета\n",
    "2. С помощтю TSNE (есть в sklearn) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n",
    "3. Визуализировать полученные двумерные представления с помощью matplotlib.scatter, покрасить разными цветами точки, соответствующие картинкам с разными атрибутами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk94C6mCsx9c"
   },
   "outputs": [],
   "source": [
    "<ваш код получения латентных представлений, применения TSNE и визуализации>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifxhsvPss5h_"
   },
   "source": [
    "Что вы думаете о виде латентного представления?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESPBHrL3YY8H"
   },
   "source": [
    "## Congrats v2.0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIYuKFwijN2U"
   },
   "source": [
    "# Conditional VAE (2 балла)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5l8Bu1RPjUx"
   },
   "source": [
    "Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n",
    "Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n",
    "И вот мне понадобилось сгенерировать цифру 8. И я подставляю разные варианты шума, и все никак не генерится восьмерка -- у меня получаются то пятерки, то тройки, то четверки. Гадость(\n",
    "\n",
    "  Хотелось бы добавить к нашему AE функцию \"выдай мне пожалуйста рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).\n",
    "  Типа я такая говорю \"выдай мне случайную восьмерку\" и оно генерит случайную восьмерку!\n",
    "\n",
    "Conditional AE -- так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n",
    "\n",
    "И в этой части проекта мы научимся такие обучать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j8zNIwKPY-6"
   },
   "source": [
    "## Архитектура\n",
    "\n",
    "На картинке ниже представлена архитектура простого Conditional AE.\n",
    "\n",
    "По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n",
    "\n",
    "То есть, в первый (входной) слой энкодера есть конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). Первый слой декодера есть конкатенация латентного вектора и информации о классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6YloFEAPeM4"
   },
   "source": [
    "\n",
    "![alt text](https://i.ibb.co/2tsWknB/Screen-Shot-2020-01-15-at-9-02-15-PM.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxg2tDSfRbLF"
   },
   "source": [
    "На всякий случай: это VAE, то есть, latent у него состоит из mu и sigma все еще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpFbSXLaPrm1"
   },
   "source": [
    "Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX0zxklMPwI2"
   },
   "source": [
    "### P.S.\n",
    "Можно ередавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wJgo_EUP0F1"
   },
   "source": [
    "### Датасет\n",
    "Здесь я предлагаю вам два вариана. Один попроще, другой -- посложнее, но поинтереснее =)\n",
    "\n",
    "1. Использовать датасет MNIST (http://yann.lecun.com/exdb/mnist/). Обучать conditional VAE на этом датасете, condition -- класс цифры. \n",
    "\n",
    "2. Использовать датасет лиц, с которым мы игрались выше. Condition -- пол/раса/улыбки/whatever из lfw_deepfinetuned.txt. \n",
    "\n",
    "Почему второй вариант \"посложнее\" -- потому что я сама еще не знаю, получится ли такой CVAE с лицами или нет =) Вы -- исследователи! (не ну это же проект, так и должно быть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar701cHOkDKS"
   },
   "outputs": [],
   "source": [
    "<тут ваш код объявления CVAE, лосса, оптимизатора и тренировки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoMw-IFyP5A2"
   },
   "source": [
    "## Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe1zWyZHkLV2"
   },
   "source": [
    "Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n",
    "Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора картинки цифры 5 и 7, а для лиц людей -- восстановить лицо улыбающегося и хмурого человека или лица людей разного пола (смотря на чем был ваш кондишен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0SQIhvNP9Dr"
   },
   "outputs": [],
   "source": [
    "<тут нужно научиться сэмплировать из декодера цифры определенного класса>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAWBu8rzQBgQ"
   },
   "source": [
    "Splendid! Вы великолепны!\n",
    "\n",
    "Ну круто же, ну?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2S77cm3O1v"
   },
   "source": [
    "## Latent Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt7x8Ek_rHTE"
   },
   "source": [
    "Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n",
    "\n",
    "Опять же, нужно покрасить точки в разные цвета в зависимости от класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSCYK7sH3KEc"
   },
   "outputs": [],
   "source": [
    "<ваш код получения латентных представлений, применения TSNE и визуализации>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8IELWu3Z2c"
   },
   "source": [
    "Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K37K3eZqaY9"
   },
   "source": [
    "# BONUS 1: Image Morphing (1 балл) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "201i9Q_-qeUy"
   },
   "source": [
    "<a href=\"https://ibb.co/rxr9YgL\"><img src=\"https://i.ibb.co/D92dhN6/1-6y-Okto2-BUp-ONJpk5x-LRMtw.png\" alt=\"1-6y-Okto2-BUp-ONJpk5x-LRMtw\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocvDBEAoqsDK"
   },
   "source": [
    "Предлагаю вам поиграться не только с улыбками, но и с получением из одного человека другого!\n",
    "\n",
    "План:\n",
    "1. Берем две картинки разных людей из датасета\n",
    "2. Получаем их латентные представления X и Y\n",
    "3. Складываем латентные представления с коэффициентом $\\alpha$:\n",
    "  $$\\alpha X + (1-\\alpha) Y$$\n",
    "  где $\\alpha$ принимает несколько значений от 0 до 1 \n",
    "4. Визуализируем, как один человек превращается в другого!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcK14nCGrQl2"
   },
   "outputs": [],
   "source": [
    "<тут ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN3D_k5W_WZz"
   },
   "source": [
    "# BONUS 2: Denoising (2 балла) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a1jkpkCsIU"
   },
   "source": [
    "У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них это бонусное задание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8EN-8jlCtmd"
   },
   "source": [
    "Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n",
    "То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1OJg6jhlaZl"
   },
   "source": [
    "<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysI0BCuRDbvm"
   },
   "source": [
    "Для того, чтобы поставить эксперимент, нужно взять ваш любимый датасет (датасет лиц или MSE с прошлых заданий или любой другой) и сделать копию этого датасета с шумом. \n",
    "\n",
    "В питоне шум можно добавить так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5e746iVDgSm"
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fSPkXMtDpd5"
   },
   "outputs": [],
   "source": [
    "<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B03NQ_sKDvg2"
   },
   "outputs": [],
   "source": [
    "<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTnyjnmVk4uK"
   },
   "source": [
    "## Бонус 2.1: Occlusion (+еще 1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41e6G2VFqAqY"
   },
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/wYrKtQY/Screen-Shot-2020-06-04-at-5-06-35-PM.png\" alt=\"Screen-Shot-2020-06-04-at-5-06-35-PM\" border=\"0\" width=\"300\" height=\"300\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRgHI3TJlAlA"
   },
   "source": [
    "Автоэнкодерами можно не только убирать шум, но и восстанавливать части картинки, которые чем-то закрыты! \n",
    "\n",
    "Эксперимент здесь такой: вместо наложения шума на картинку, \"закрываем\" часть картинки заплаткой и тренируем AE/VAE восстанавливать закрытую часть картинки.\n",
    "\n",
    "Важно, чтобы заплатка была не очень большая. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac7l0_EiqWR3"
   },
   "outputs": [],
   "source": [
    "<тут ваш код обучения автоэнкодера на occluded (простите, не знаю, как это сказать по-русски) картинках. Не забудтье разбить на train/test!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDiCPYLm2bY"
   },
   "source": [
    "# Bonus 3: Image Retrieval (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xao_27WMm7AL"
   },
   "source": [
    "Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__bdS23ndeY"
   },
   "source": [
    "План:\n",
    "\n",
    "1. Получаем латентные представления всех лиц тренировочного датасета\n",
    "2. Обучаем на них LSHForest (sklearn.neighbors.LSHForest), например, с n_estimators=50\n",
    "3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n",
    "4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n",
    "5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IksC2ucIoND-"
   },
   "source": [
    "Немного кода вам в помощь: (feel free to delete everything and write your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK0YpLMRoEa0"
   },
   "outputs": [],
   "source": [
    "codes = <поучите латентные представления картинок из трейна>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KisDrgZdoWdt"
   },
   "outputs": [],
   "source": [
    "# обучаем LSHForest\n",
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_S5zPb5obam"
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n",
    "  # прогоняет векторы через декодер и получает картинки ближайших людей\n",
    "\n",
    "  code = <получение латентного представления image>\n",
    "    \n",
    "  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n",
    "\n",
    "  return distances, X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2kjV5wupLP_"
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "\n",
    "  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Ja1UNf_oJq"
   },
   "outputs": [],
   "source": [
    "<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Z4M1J9OXss"
   },
   "source": [
    "# Bonus 4: Телеграм-бот (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZe_urxSOb9E"
   },
   "source": [
    "Вы можете написать телеграм-бота с функционалом AE. Например, он может добавлять к вашей фотографии улыбку или искать похожие на ваше лицо лица среди лиц датасета. \n",
    "\n",
    "Код бота должно быть можно проверить!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-KAfd5pLCBS"
   },
   "source": [
    "# Эпилог"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDI06AONLYKG"
   },
   "source": [
    "здесь мы рассмотрели не все применения автоэнкодеров. Еще есть, например:\n",
    "\n",
    "-- поиск аномалий\n",
    "-- дополнение отсутствующих частей картины\n",
    "-- работа с sequential данными (например, временными рядами)\n",
    "-- гибриды ГАН+АЕ, которые активно изучаются в последнее время\n",
    "-- использование латентных переменных АЕ в качестве фичей\n",
    "...\n",
    "\n",
    "Они не были частью этого проекта, потому что для их реализации пришлось бы больше возиться с датасетами. \n",
    "\n",
    "Но! Если вы хотите, вы, конечно, всегда можете реализовать еще что-то и получить за это еще допбаллы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBCiWOicLwLI"
   },
   "source": [
    "Надеюсь, вам понравилось!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Wru2LNFuL2Iq",
    "z9CC-DUhYY7i",
    "dFi96giuYY7t",
    "Ey8dD9s0YY7w",
    "yIYuKFwijN2U",
    "6K37K3eZqaY9",
    "KN3D_k5W_WZz",
    "-NDiCPYLm2bY",
    "M-KAfd5pLCBS"
   ],
   "name": "Autoencoders.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
